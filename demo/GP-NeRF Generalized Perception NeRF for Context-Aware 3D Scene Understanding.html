<!doctype html>
<html lang="zh-CN">
<head>
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding - Vision Intelligence Research Group</title>
	
	<meta name='robots' content='max-image-preview:large' />
	<meta property="og:type" content="website" />
	<meta property="og:title" content="Vision Intelligence Research Group" />
	
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=Playfair+Display:wght@600;700;800&display=swap" rel="stylesheet">
	
	<style>
		/* ==================== ÂÖ®Â±ÄÂèòÈáè ==================== */
		:root {
			--primary-color: #2563eb;
			--primary-dark: #1e40af;
			--accent-color: #10b981;
			
			--text-white: #ffffff;
			--text-dark: #1e293b;
			--text-gray: #64748b;
			--text-light: #94a3b8;
			
			--bg-white: #ffffff;
			--bg-light: #f8fafc;
			--bg-dark: #0f172a;
			
			--border-color: #e2e8f0;
			
			--shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
			--shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
			--shadow-lg: 0 20px 25px -5px rgba(0, 0, 0, 0.1);
			
			--transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
		}
		
		* {
			margin: 0;
			padding: 0;
			box-sizing: border-box;
		}
		
		body {
			font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
			color: var(--text-dark);
			line-height: 1.8;
			overflow-x: hidden;
			background: var(--bg-light);
		}
		
		/* ==================== ÂØºËà™Ê†è ==================== */
		.navbar {
			background: rgba(255, 255, 255, 0.98);
			backdrop-filter: blur(10px);
			box-shadow: var(--shadow-sm);
			position: sticky;
			top: 0;
			left: 0;
			right: 0;
			z-index: 1000;
			padding: 1rem 2rem;
			transition: var(--transition);
		}
		
		.nav-container {
			max-width: 1400px;
			margin: 0 auto;
			display: flex;
			justify-content: space-between;
			align-items: center;
		}
		
		.logo {
			font-family: 'Playfair Display', serif;
			font-size: 1.5rem;
			font-weight: 800;
			color: var(--text-dark);
			text-decoration: none;
			transition: var(--transition);
		}
		
		.nav-menu {
			display: flex;
			gap: 2.5rem;
			list-style: none;
			align-items: center;
		}
		
		.nav-menu a {
			color: var(--text-gray);
			text-decoration: none;
			font-weight: 500;
			font-size: 0.95rem;
			transition: var(--transition);
			position: relative;
		}
		
		.nav-menu a:hover {
			color: var(--primary-color);
		}
		
		.nav-menu a::after {
			content: '';
			position: absolute;
			bottom: -5px;
			left: 0;
			width: 0;
			height: 2px;
			background: var(--primary-color);
			transition: width 0.3s ease;
		}
		
		.nav-menu a:hover::after {
			width: 100%;
		}
		
		/* ==================== ‰∏ªÂÆπÂô® ==================== */
		.main-container {
			max-width: 1200px;
			margin: 0 auto;
			padding: 2rem;
		}
		
				/* ==================== ËøîÂõûÊåâÈíÆ ==================== */
		.back-button-container {
			position: absolute;
			left: -80px;
			top: 3rem;
		}
		
		.back-button {
			display: inline-flex;
			align-items: center;
			justify-content: center;
			width: 60px;
			height: 60px;
			background: var(--bg-white);
			border: 2px solid var(--border-color);
			border-radius: 50%;
			color: var(--text-gray);
			text-decoration: none;
			transition: var(--transition);
			box-shadow: var(--shadow-md);
		}
		
		.back-button:hover {
			background: var(--primary-color);
			color: var(--text-white);
			border-color: var(--primary-color);
			transform: scale(1.1);
			box-shadow: var(--shadow-lg);
		}
		
		.back-button svg {
			transition: var(--transition);
		}
		
		.back-button:hover svg {
			transform: translateX(-3px);
		}
		
		.back-button span {
			display: none;
		}
		
		/* ==================== È°πÁõÆÊ†áÈ¢òÂå∫ ==================== */
		.project-header {
			background: var(--bg-white);
			border-radius: 16px;
			padding: 3rem;
			margin-bottom: 2rem;
			box-shadow: var(--shadow-md);
			border-left: 6px solid var(--primary-color);
		position: relative;
		}
		
		.project-title {
			font-family: 'Playfair Display', serif;
			font-size: 2.5rem;
			font-weight: 800;
			color: var(--text-dark);
			margin-bottom: 1.5rem;
			line-height: 1.3;
		}
		
		.project-meta {
			display: flex;
			align-items: center;
			gap: 2rem;
			margin-bottom: 1.5rem;
			flex-wrap: wrap;
		}
		
		.project-author {
			display: flex;
			align-items: center;
			gap: 0.5rem;
			color: var(--text-gray);
			font-size: 1rem;
			font-weight: 600;
		}
		
		.project-date {
			color: var(--text-light);
			font-size: 0.9rem;
		}
		
		.project-venue {
			display: inline-block;
			padding: 0.5rem 1.25rem;
			background: linear-gradient(135deg, var(--primary-color), #1d4ed8);
			color: white;
			border-radius: 25px;
			font-weight: 700;
			font-size: 0.95rem;
			letter-spacing: 0.5px;
		}
		
		/* ==================== È°πÁõÆÂÜÖÂÆπ ==================== */
		.project-content {
			background: var(--bg-white);
			border-radius: 16px;
			padding: 3rem;
			margin-bottom: 2rem;
			box-shadow: var(--shadow-sm);
			line-height: 2;
		}
		
		.project-content h2 {
			font-family: 'Playfair Display', serif;
			font-size: 1.75rem;
			font-weight: 700;
			color: var(--text-dark);
			margin-top: 2rem;
			margin-bottom: 1rem;
			padding-bottom: 0.5rem;
			border-bottom: 2px solid var(--border-color);
		}
		
		.project-content h2:first-child {
			margin-top: 0;
		}
		
		.project-content p {
			color: var(--text-gray);
			font-size: 1.05rem;
			margin-bottom: 1.5rem;
			text-align: justify;
		}
		
		.project-content strong {
			color: var(--text-dark);
			font-weight: 600;
		}
		
		/* ==================== È°πÁõÆÈìæÊé• ==================== */
		.project-links {
			background: linear-gradient(135deg, #f8fafc, #e0e7ff);
			border-radius: 16px;
			padding: 2rem;
			margin-bottom: 2rem;
			display: flex;
			gap: 1rem;
			flex-wrap: wrap;
			justify-content: center;
		}
		
		.project-link {
			display: inline-flex;
			align-items: center;
			gap: 0.5rem;
			padding: 0.875rem 1.75rem;
			border-radius: 8px;
			font-size: 1rem;
			font-weight: 600;
			text-decoration: none;
			transition: var(--transition);
			box-shadow: var(--shadow-sm);
		}
		
		.link-pdf {
			background: linear-gradient(135deg, var(--primary-color), #1d4ed8);
			color: white;
		}
		
		.link-pdf:hover {
			background: var(--primary-dark);
			transform: translateY(-3px);
			box-shadow: var(--shadow-md);
		}
		
		.link-code {
			background: linear-gradient(135deg, var(--accent-color), #059669);
			color: white;
		}
		
		.link-code:hover {
			background: #047857;
			transform: translateY(-3px);
			box-shadow: var(--shadow-md);
		}
		
		/* ==================== È°πÁõÆÂõæÁâá ==================== */
		.project-image {
			background: var(--bg-white);
			border-radius: 16px;
			padding: 2rem;
			margin-bottom: 2rem;
			box-shadow: var(--shadow-sm);
			text-align: center;
		}
		
		.project-image img {
			max-width: 100%;
			height: auto;
			border-radius: 12px;
			box-shadow: var(--shadow-md);
			transition: var(--transition);
		}
		
		.project-image img:hover {
			transform: scale(1.02);
			box-shadow: var(--shadow-lg);
		}
		
		/* ==================== È°µËÑö ==================== */
		.footer {
			background: var(--bg-dark);
			color: var(--text-white);
			padding: 4rem 2rem 2rem;
			margin-top: 4rem;
		}
		
		.footer-container {
			max-width: 1400px;
			margin: 0 auto;
			text-align: center;
		}
		
		.footer-container p {
			color: var(--text-light);
			margin: 0.5rem 0;
		}
		
		.footer-container a {
			color: var(--accent-color);
			text-decoration: none;
		}
		
		/* ==================== ÂìçÂ∫îÂºè ==================== */
		@media (max-width: 768px) {		
		.back-button-container {
			position: static;
			transform: none;
			margin-bottom: 1rem;
		}
		
		.back-button {
			width: 50px;
			height: 50px;
		}
		
			.nav-menu {
				display: none;
			}
			
			.project-header {
				padding: 2rem;
			}
			
			.project-title {
				font-size: 1.75rem;
			}
			
			.project-content {
				padding: 2rem;
			}
			
			.project-meta {
				flex-direction: column;
				align-items: flex-start;
				gap: 1rem;
			}
		}
	</style>
</head>

<body>
	<!-- ÂØºËà™Ê†è -->
	<nav class="navbar">
		<div class="nav-container">
			<a href="../index.html" class="logo">Vision Intelligence Group</a>
			<ul class="nav-menu">
				<li><a href="../index.html">Home</a></li>
				<li><a href="../pages/people.html">People</a></li>
				<li><a href="../pages/publications.html">Publications</a></li>
				<li><a href="../pages/demo.html">Projects</a></li>
				<li><a href="../pages/news.html">News</a></li>
				<li><a href="../pages/event.html">Events</a></li>
			</ul>
		</div>
	</nav>

	<!-- ‰∏ªÂÜÖÂÆπÂå∫ -->
	<div class="main-container">
		<!-- È°πÁõÆÊ†áÈ¢ò -->
		<div class="project-header">
			<!-- ËøîÂõûÊåâÈíÆ -->
			<div class="back-button-container">
				<a href="../pages/demo.html" class="back-button" title="Back to Projects">
					<svg viewBox="0 0 24 24" width="24" height="24" fill="currentColor">
						<path d="M20 11H7.83l5.59-5.59L12 4l-8 8 8 8 1.41-1.41L7.83 13H20v-2z"/>
					</svg>
					<span>Back to Projects</span>
				</a>
			</div>
			<h1 class="project-title">GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding</h1>
			<div class="project-meta">
				<div class="project-author">
					üë§ <span>Hao Li</span>
				</div>
				<div class="project-date">
					üìÖ Last updated on May 22, 2024
				</div>
				<span class="project-venue">CVPR</span>
			</div>
		</div>
		
		<!-- È°πÁõÆÂõæÁâá -->
		<div class="project-image">
			<img src="../public/images/project/GP-NeRF- Generalized Perception NeRF for Context-Aware 3D Scene Understanding.png" alt="GP-NeRF Framework">
		</div>
		
	<!-- È°πÁõÆÈìæÊé• -->
	<div class="project-links">
		<a href="https://arxiv.org/pdf/2311.11863.pdf" target="_blank" class="project-link link-pdf">üìÑ PDF</a>
		<a href="https://vision-intelligence.com.cn/%e2%80%98https:/github.com/lifuguan/GP-NeRF%27" target="_blank" class="project-link link-code">üíª Code</a>
	</div>
		
		<!-- È°πÁõÆÂÜÖÂÆπ -->
		<div class="project-content">
			<h2>Motivation</h2>
			<p>
				Applying NeRF to downstream perception tasks for scene understanding and representation is becoming increasingly popular. Most existing methods treat semantic prediction as an additional rendering task, i.e., the <strong>"label rendering"</strong> task, to build semantic NeRFs.
			</p>
			<p>
				However, by rendering semantic/instance labels per pixel without considering the contextual information of the rendered image, these methods usually suffer from <strong>unclear boundary segmentation</strong> and <strong>abnormal segmentation of pixels</strong> within an object.
			</p>
			
			<h2>Proposed Solution</h2>
			<p>
				To solve this problem, we propose <strong>Generalized Perception NeRF (GP-NeRF)</strong>, a novel pipeline that makes the widely used segmentation model and NeRF work compatibly under a unified framework, for facilitating context-aware 3D scene perception.
			</p>
			<p>
				To accomplish this goal, we introduce <strong>transformers</strong> to aggregate radiance as well as semantic embedding fields jointly for novel views and facilitate the joint volumetric rendering of both fields.
			</p>
			<p>
				In addition, we propose two self-distillation mechanisms:
			</p>
			<p>
				<strong>1. Semantic Distill Loss:</strong> Enhances the discrimination and quality of the semantic field.
			</p>
			<p>
				<strong>2. Depth-Guided Semantic Distill Loss:</strong> Maintains geometric consistency between radiance and semantic fields.
			</p>
			
			<h2>Experimental Results</h2>
			<p>
				In evaluation, we conduct experimental comparisons under two perception tasks (i.e., <strong>semantic segmentation</strong> and <strong>instance segmentation</strong>) using both synthetic and real-world datasets.
			</p>
			<p>
				Notably, our method outperforms SOTA approaches by:
			</p>
			<p>
				‚Ä¢ <strong>6.94%</strong> on generalized semantic segmentation<br>
				‚Ä¢ <strong>11.76%</strong> on finetuning semantic segmentation<br>
				‚Ä¢ <strong>8.47%</strong> on instance segmentation
			</p>
			<p>
				These significant improvements demonstrate the effectiveness of GP-NeRF in bridging the gap between neural radiance fields and context-aware perception, enabling more accurate and consistent 3D scene understanding.
			</p>
		</div>
	</div>
	
	<!-- È°µËÑö -->
	<footer class="footer">
		<div class="footer-container">
			<p>Copyright ¬© 2025 <a href="../index.html">Brain Lab</a>. All rights reserved.</p>
			<p style="font-size: 0.9rem; margin-top: 0.75rem;">
				ËÑë‰∏é‰∫∫Â∑•Êô∫ËÉΩÂÆûÈ™åÂÆ§ | Vision Intelligence Research Group
			</p>
		</div>
	</footer>
</body>
</html>

