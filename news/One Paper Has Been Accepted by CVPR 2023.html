<!doctype html>
<html lang="zh-CN">
<head>
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>One Paper Has Been Accepted by CVPR 2023 - Vision Intelligence Research Group</title>
	
	<meta name='robots' content='max-image-preview:large' />
	<meta property="og:type" content="website" />
	<meta property="og:title" content="Vision Intelligence Research Group" />
	
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=Playfair+Display:wght@600;700;800&display=swap" rel="stylesheet">
	
	<style>
		/* ==================== ÂÖ®Â±ÄÂèòÈáè ==================== */
		:root {
			--primary-color: #2563eb;
			--primary-dark: #1e40af;
			--accent-color: #10b981;
			
			--text-white: #ffffff;
			--text-dark: #1e293b;
			--text-gray: #64748b;
			--text-light: #94a3b8;
			
			--bg-white: #ffffff;
			--bg-light: #f8fafc;
			--bg-dark: #0f172a;
			
			--border-color: #e2e8f0;
			
			--shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
			--shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
			--shadow-lg: 0 20px 25px -5px rgba(0, 0, 0, 0.1);
			
			--transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
		}
		
		* {
			margin: 0;
			padding: 0;
			box-sizing: border-box;
		}
		
		body {
			font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
			color: var(--text-dark);
			line-height: 1.8;
			overflow-x: hidden;
			background: var(--bg-light);
		}
		
		/* ==================== ÂØºËà™Ê†è ==================== */
		.navbar {
			background: rgba(255, 255, 255, 0.98);
			backdrop-filter: blur(10px);
			box-shadow: var(--shadow-sm);
			position: sticky;
			top: 0;
			left: 0;
			right: 0;
			z-index: 1000;
			padding: 1rem 2rem;
			transition: var(--transition);
		}
		
		.nav-container {
			max-width: 1400px;
			margin: 0 auto;
			display: flex;
			justify-content: space-between;
			align-items: center;
		}
		
		.logo {
			font-family: 'Playfair Display', serif;
			font-size: 1.5rem;
			font-weight: 800;
			color: var(--text-dark);
			text-decoration: none;
			transition: var(--transition);
		}
		
		.nav-menu {
			display: flex;
			gap: 2.5rem;
			list-style: none;
			align-items: center;
		}
		
		.nav-menu a {
			color: var(--text-gray);
			text-decoration: none;
			font-weight: 500;
			font-size: 0.95rem;
			transition: var(--transition);
			position: relative;
		}
		
		.nav-menu a:hover {
			color: var(--primary-color);
		}
		
	/* ==================== ‰∏ªÂÆπÂô® ==================== */
	.main-container {
		max-width: 1200px;
		margin: 0 auto;
		padding: 3rem 2rem;
	}
		
	/* ==================== ËøîÂõûÊåâÈíÆ ==================== */
	.back-button-container {
		position: absolute;
		left: -80px;
		top: 3rem;
	}
	
	.back-button {
		display: inline-flex;
		align-items: center;
		justify-content: center;
		width: 60px;
		height: 60px;
		background: var(--bg-white);
		border: 2px solid var(--border-color);
		border-radius: 50%;
		color: var(--text-gray);
		text-decoration: none;
		transition: var(--transition);
		box-shadow: var(--shadow-md);
	}
	
	.back-button:hover {
		background: var(--primary-color);
		color: var(--text-white);
		border-color: var(--primary-color);
		transform: scale(1.1);
		box-shadow: var(--shadow-lg);
	}
	
	.back-button svg {
		transition: var(--transition);
	}
	
	.back-button:hover svg {
		transform: translateX(-3px);
	}
	
	.back-button span {
		display: none;
	}
		
	/* ==================== Êñ∞ÈóªÂ§¥ÈÉ® ==================== */
	.news-header {
		background: white;
		padding: 2.5rem;
		border-radius: 16px;
		box-shadow: var(--shadow-md);
		margin-bottom: 2rem;
		position: relative;
	}
		
		.news-title {
			font-family: 'Playfair Display', serif;
			font-size: 2.5rem;
			font-weight: 800;
			color: var(--text-dark);
			line-height: 1.3;
			margin-bottom: 1.5rem;
		}
		
		.news-meta {
			display: flex;
			flex-wrap: wrap;
			gap: 1.5rem;
			color: var(--text-gray);
			font-size: 0.95rem;
		}
		
		.news-date {
			display: flex;
			align-items: center;
			gap: 0.5rem;
		}
		
		.news-author {
			display: flex;
			align-items: center;
			gap: 0.5rem;
		}
		
		.news-badge {
			display: inline-block;
			padding: 0.5rem 1.25rem;
			background: linear-gradient(135deg, #f59e0b, #d97706);
			color: white;
			border-radius: 25px;
			font-weight: 700;
			font-size: 0.85rem;
			letter-spacing: 0.5px;
		}
		
		/* ==================== Êñ∞ÈóªÂõæÁâá ==================== */
		.news-image {
			width: 100%;
			margin-bottom: 2rem;
			border-radius: 16px;
			overflow: hidden;
			box-shadow: var(--shadow-md);
		}
		
		.news-image img {
			width: 100%;
			height: auto;
			display: block;
		}
		
		/* ==================== Êñ∞ÈóªÂÜÖÂÆπ ==================== */
		.news-content {
			background: white;
			padding: 2.5rem;
			border-radius: 16px;
			box-shadow: var(--shadow-md);
		}
		
		.news-content h2 {
			font-family: 'Playfair Display', serif;
			font-size: 1.75rem;
			font-weight: 700;
			color: var(--text-dark);
			margin: 2rem 0 1rem 0;
			padding-bottom: 0.5rem;
			border-bottom: 3px solid var(--primary-color);
		}
		
		.news-content h2:first-child {
			margin-top: 0;
		}
		
		.news-content h3 {
			font-size: 1.4rem;
			font-weight: 600;
			color: var(--text-dark);
			margin: 1.5rem 0 1rem 0;
		}
		
		.news-content p {
			margin-bottom: 1.25rem;
			color: var(--text-gray);
			font-size: 1.05rem;
		}
		
		.news-content strong {
			color: var(--text-dark);
			font-weight: 600;
		}
		
		.news-content a {
			color: var(--primary-color);
			text-decoration: none;
			transition: var(--transition);
		}
		
		.news-content a:hover {
			color: var(--primary-dark);
			text-decoration: underline;
		}
		
		.paper-info {
			background: linear-gradient(135deg, #eff6ff, #dbeafe);
			padding: 1.5rem;
			border-radius: 12px;
			border-left: 4px solid var(--primary-color);
			margin: 1.5rem 0;
		}
		
		.paper-info h3 {
			color: var(--primary-dark);
			margin-top: 0;
			font-size: 1.3rem;
		}
		
		.paper-info p {
			margin-bottom: 0.5rem;
		}
		
		/* ==================== È°µËÑö ==================== */
		.footer {
			background: var(--bg-dark);
			color: var(--text-white);
			padding: 3rem 2rem;
			margin-top: 4rem;
			text-align: center;
		}
		
		.footer-container {
			max-width: 1400px;
			margin: 0 auto;
		}
		
		.footer-container p {
			color: var(--text-light);
			margin: 0.5rem 0;
		}
		
		.footer-container a {
			color: var(--accent-color);
			text-decoration: none;
		}
		
		/* ==================== ÂìçÂ∫îÂºèËÆæËÆ° ==================== */
		@media (max-width: 768px) {
			.news-title {
				font-size: 2rem;
			}
			
			.news-meta {
				flex-direction: column;
				gap: 0.75rem;
			}
			
		.main-container {
			padding: 2rem 1rem;
		}
		
		.back-button-container {
			position: static;
			transform: none;
			margin-bottom: 1rem;
		}
		
		.back-button {
			width: 50px;
			height: 50px;
		}
			
			.news-header,
			.news-content {
				padding: 1.5rem;
			}
		}
	</style>
</head>
<body>
	<!-- ÂØºËà™Ê†è -->
	<nav class="navbar" id="navbar">
		<div class="nav-container">
			<a href="../home.html" class="logo">Vision Intelligence Group</a>
						<ul class="nav-menu">
				<li><a href="../home.html">Home</a></li>
				<li><a href="../pages/people.html">People</a></li>
				<li><a href="../pages/publications.html">Publications</a></li>
				<li><a href="../pages/demo.html">Projects</a></li>
				<li><a href="../pages/news.html">News</a></li>
				<li><a href="../pages/event.html">Events</a></li>
			</ul>
		</div>
	</nav>
	
<div class="main-container">
	<!-- Êñ∞ÈóªÊ†áÈ¢ò -->
	<div class="news-header">
		<!-- ËøîÂõûÊåâÈíÆ -->
		<div class="back-button-container">
			<a href="../pages/news.html" class="back-button" title="Back to News">
				<svg viewBox="0 0 24 24" width="24" height="24" fill="currentColor">
					<path d="M20 11H7.83l5.59-5.59L12 4l-8 8 8 8 1.41-1.41L7.83 13H20v-2z"/>
				</svg>
				<span>Back to News</span>
			</a>
		</div>
		<h1 class="news-title">üéâ One Paper Has Been Accepted by CVPR 2023</h1>
		<div class="news-meta">
			<div class="news-date">
				üìÖ <span>March 24, 2023</span>
			</div>
			<div class="news-author">
				‚è±Ô∏è <span>1 min read</span>
			</div>
			<span class="news-badge">CVPR 2023</span>
		</div>
	</div>
		
		<!-- Êñ∞ÈóªÂõæÁâá -->
		<div class="news-image">
			<img src="../public/images/news/One Paper Has Been Accepted by CVPR 2023.webp" alt="CVPR 2023">
		</div>
		
		<!-- Êñ∞ÈóªÂÜÖÂÆπ -->
		<div class="news-content">
			<p>
				<strong>CVPR 2023</strong> officially released the list of accepted papers. We are thrilled to announce that Dr. Li's paper from our team has been included!
			</p>
			
			<div class="paper-info">
				<h3>üìÑ Boosting Low-Data Instance Segmentation by Unsupervised Pre-training with Saliency Prompt</h3>
				<p><strong>Authors:</strong> Hao Li (ÊùéÊòä), Dingwen Zhang (Âº†ÈºéÊñá), Junwei Han (Èü©ÂÜõ‰ºü)</p>
				<p><strong>Conference:</strong> CVPR 2023</p>
			</div>
			
			<h2>Research Background</h2>
			<p>
				Recently, inspired by DETR variants, <strong>query-based end-to-end instance segmentation (QEIS)</strong> methods have outperformed CNN-based models on large-scale datasets.
			</p>
			<p>
				Yet they would <strong>lose efficacy when only a small amount of training data is available</strong> since it's hard for the crucial queries/kernels to learn localization and shape priors.
			</p>
			
			<h2>Key Contributions</h2>
			<p>
				To this end, this work offers a novel <strong>unsupervised pre-training solution for low-data regimes</strong>. Inspired by the recent success of the Prompting technique, we introduce a new pre-training method that boosts QEIS models by giving <strong>Saliency Prompt</strong> for queries/kernels.
			</p>
			
			<h3>Our method contains three parts:</h3>
			
			<h3>1. Saliency Masks Proposal</h3>
			<p>
				This component is responsible for <strong>generating pseudo masks from unlabeled images</strong> based on the saliency mechanism. By leveraging visual saliency, we can automatically identify and segment prominent objects without manual annotation.
			</p>
			
			<h3>2. Prompt-Kernel Matching</h3>
			<p>
				This part <strong>transfers pseudo masks into prompts</strong> and injects the corresponding localization and shape priors to the best-matched kernels. This innovative approach allows the model to learn from unlabeled data effectively.
			</p>
			
			<h3>3. Kernel Supervision</h3>
			<p>
				Kernel Supervision is applied to supply <strong>supervision at the kernel level</strong> for robust learning. This ensures that each kernel learns meaningful and discriminative features.
			</p>
			
			<h2>Practical Impact</h2>
			<p>
				From a practical perspective, our pre-training method helps QEIS models achieve <strong>similar convergence speed and comparable performance with CNN-based models</strong> in low-data regimes.
			</p>
			<p>
				This is a significant breakthrough as it enables state-of-the-art instance segmentation models to work effectively even when training data is scarce, which is a common scenario in many real-world applications.
			</p>
			
			<h2>Experimental Results</h2>
			<p>
				Experimental results show that our method <strong>significantly boosts several QEIS models on three datasets</strong>. The comprehensive evaluation demonstrates the effectiveness and generalizability of our proposed approach across different architectures and datasets.
			</p>
			<p>
				The results validate that saliency-based unsupervised pre-training is a promising direction for addressing the low-data challenge in instance segmentation tasks.
			</p>
			
			<h2>Conclusion</h2>
			<p>
				This acceptance at CVPR 2023 represents a significant contribution to the instance segmentation field. By introducing saliency prompts for unsupervised pre-training, our work bridges the gap between query-based and CNN-based methods in low-data scenarios.
			</p>
			<p>
				Congratulations to Dr. Hao Li (ÊùéÊòä) and all co-authors for this outstanding achievement! üéä
			</p>
		</div>
	</div>
	
	<!-- È°µËÑö -->
	<footer class="footer">
		<div class="footer-container">
			<p>Copyright ¬© 2025 <a href="../home.html">Brain Lab</a>. All rights reserved.</p>
			<p style="font-size: 0.9rem; margin-top: 0.75rem;">
				ËÑë‰∏é‰∫∫Â∑•Êô∫ËÉΩÂÆûÈ™åÂÆ§ | Vision Intelligence Research Group
			</p>
		</div>
	</footer>
</body>
</html>


