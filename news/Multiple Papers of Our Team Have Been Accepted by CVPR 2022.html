<!doctype html>
<html lang="zh-CN">
<head>
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Multiple Papers of Our Team Have Been Accepted by CVPR 2022 - Vision Intelligence Research Group</title>
	
	<meta name='robots' content='max-image-preview:large' />
	<meta property="og:type" content="website" />
	<meta property="og:title" content="Vision Intelligence Research Group" />
	
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=Playfair+Display:wght@600;700;800&display=swap" rel="stylesheet">
	
	<style>
		/* ==================== å…¨å±€å˜é‡ ==================== */
		:root {
			--primary-color: #2563eb;
			--primary-dark: #1e40af;
			--accent-color: #10b981;
			
			--text-white: #ffffff;
			--text-dark: #1e293b;
			--text-gray: #64748b;
			--text-light: #94a3b8;
			
			--bg-white: #ffffff;
			--bg-light: #f8fafc;
			--bg-dark: #0f172a;
			
			--border-color: #e2e8f0;
			
			--shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
			--shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
			--shadow-lg: 0 20px 25px -5px rgba(0, 0, 0, 0.1);
			
			--transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
		}
		
		* {
			margin: 0;
			padding: 0;
			box-sizing: border-box;
		}
		
		body {
			font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
			color: var(--text-dark);
			line-height: 1.8;
			overflow-x: hidden;
			background: var(--bg-light);
		}
		
		/* ==================== å¯¼èˆªæ  ==================== */
		.navbar {
			background: rgba(255, 255, 255, 0.98);
			backdrop-filter: blur(10px);
			box-shadow: var(--shadow-sm);
			position: sticky;
			top: 0;
			left: 0;
			right: 0;
			z-index: 1000;
			padding: 1rem 2rem;
			transition: var(--transition);
		}
		
		.nav-container {
			max-width: 1400px;
			margin: 0 auto;
			display: flex;
			justify-content: space-between;
			align-items: center;
		}
		
		.logo {
			font-family: 'Playfair Display', serif;
			font-size: 1.5rem;
			font-weight: 800;
			color: var(--text-dark);
			text-decoration: none;
			transition: var(--transition);
		}
		
		.nav-menu {
			display: flex;
			gap: 2.5rem;
			list-style: none;
			align-items: center;
		}
		
		.nav-menu a {
			color: var(--text-gray);
			text-decoration: none;
			font-weight: 500;
			font-size: 0.95rem;
			transition: var(--transition);
			position: relative;
		}
		
		.nav-menu a:hover {
			color: var(--primary-color);
		}
		
		/* ==================== ä¸»å®¹å™¨ ==================== */
		.main-container {
			max-width: 1200px;
			margin: 0 auto;
			padding: 3rem 2rem;
		}
		
		/* ==================== è¿”å›æŒ‰é’® ==================== */
		.back-button-container {
			position: absolute;
			left: -80px;
			top: 3rem;
		}
		
		.back-button {
			display: inline-flex;
			align-items: center;
			justify-content: center;
			width: 60px;
			height: 60px;
			background: var(--bg-white);
			border: 2px solid var(--border-color);
			border-radius: 50%;
			color: var(--text-gray);
			text-decoration: none;
			transition: var(--transition);
			box-shadow: var(--shadow-md);
		}
		
		.back-button:hover {
			background: var(--primary-color);
			color: var(--text-white);
			border-color: var(--primary-color);
			transform: scale(1.1);
			box-shadow: var(--shadow-lg);
		}
		
		.back-button svg {
			transition: var(--transition);
		}
		
		.back-button:hover svg {
			transform: translateX(-3px);
		}
		
		.back-button span {
			display: none;
		}
		
		/* ==================== æ–°é—»å¤´éƒ¨ ==================== */
		.news-header {
			background: white;
			padding: 2.5rem;
			border-radius: 16px;
			box-shadow: var(--shadow-md);
			margin-bottom: 2rem;
			position: relative;
		}
		
		.news-title {
			font-family: 'Playfair Display', serif;
			font-size: 2.5rem;
			font-weight: 800;
			color: var(--text-dark);
			line-height: 1.3;
			margin-bottom: 1.5rem;
		}
		
		.news-meta {
			display: flex;
			flex-wrap: wrap;
			gap: 1.5rem;
			color: var(--text-gray);
			font-size: 0.95rem;
		}
		
		.news-date {
			display: flex;
			align-items: center;
			gap: 0.5rem;
		}
		
		.news-author {
			display: flex;
			align-items: center;
			gap: 0.5rem;
		}
		
		.news-badge {
			display: inline-block;
			padding: 0.5rem 1.25rem;
			background: linear-gradient(135deg, #f59e0b, #d97706);
			color: white;
			border-radius: 25px;
			font-weight: 700;
			font-size: 0.85rem;
			letter-spacing: 0.5px;
		}
		
		/* ==================== æ–°é—»å›¾ç‰‡ ==================== */
		.news-image {
			width: 100%;
			margin-bottom: 2rem;
			border-radius: 16px;
			overflow: hidden;
			box-shadow: var(--shadow-md);
		}
		
		.news-image img {
			width: 100%;
			height: auto;
			display: block;
		}
		
		/* ==================== æ–°é—»å†…å®¹ ==================== */
		.news-content {
			background: white;
			padding: 2.5rem;
			border-radius: 16px;
			box-shadow: var(--shadow-md);
		}
		
		.news-content h2 {
			font-family: 'Playfair Display', serif;
			font-size: 1.75rem;
			font-weight: 700;
			color: var(--text-dark);
			margin: 2rem 0 1rem 0;
			padding-bottom: 0.5rem;
			border-bottom: 3px solid var(--primary-color);
		}
		
		.news-content h2:first-child {
			margin-top: 0;
		}
		
		.news-content h3 {
			font-size: 1.4rem;
			font-weight: 600;
			color: var(--text-dark);
			margin: 1.5rem 0 1rem 0;
		}
		
		.news-content p {
			margin-bottom: 1.25rem;
			color: var(--text-gray);
			font-size: 1.05rem;
		}
		
		.news-content strong {
			color: var(--text-dark);
			font-weight: 600;
		}
		
		.news-content ul {
			margin-left: 2rem;
			margin-bottom: 1.25rem;
		}
		
		.news-content li {
			margin-bottom: 0.5rem;
		}
		
		.paper-info {
			background: linear-gradient(135deg, #eff6ff, #dbeafe);
			padding: 1.5rem;
			border-radius: 12px;
			border-left: 4px solid var(--primary-color);
			margin: 1.5rem 0;
		}
		
		.paper-info h3 {
			color: var(--primary-dark);
			margin-top: 0;
			font-size: 1.3rem;
		}
		
		.paper-info p {
			margin-bottom: 0.5rem;
		}
		
		/* ==================== é¡µè„š ==================== */
		.footer {
			background: var(--bg-dark);
			color: var(--text-white);
			padding: 3rem 2rem;
			margin-top: 4rem;
			text-align: center;
		}
		
		.footer-container {
			max-width: 1400px;
			margin: 0 auto;
		}
		
		.footer-container p {
			color: var(--text-light);
			margin: 0.5rem 0;
		}
		
		.footer-container a {
			color: var(--accent-color);
			text-decoration: none;
		}
		
		/* ==================== å“åº”å¼è®¾è®¡ ==================== */
		@media (max-width: 768px) {
			.news-title {
				font-size: 2rem;
			}
			
			.news-meta {
				flex-direction: column;
				gap: 0.75rem;
			}
			
			.main-container {
				padding: 2rem 1rem;
			}
			
			.back-button-container {
				position: static;
				transform: none;
				margin-bottom: 1rem;
			}
			
			.back-button {
				width: 50px;
				height: 50px;
			}
			
			.news-header,
			.news-content {
				padding: 1.5rem;
			}
		}
	</style>
</head>
<body>
	<!-- å¯¼èˆªæ  -->
	<nav class="navbar" id="navbar">
		<div class="nav-container">
			<a href="../index.html" class="logo">Vision Intelligence Group</a>
						<ul class="nav-menu">
				<li><a href="../index.html">Home</a></li>
				<li><a href="../pages/people.html">People</a></li>
				<li><a href="../pages/publications.html">Publications</a></li>
				<li><a href="../pages/demo.html">Projects</a></li>
				<li><a href="../pages/news.html">News</a></li>
				<li><a href="../pages/event.html">Events</a></li>
			</ul>
		</div>
	</nav>
	
	<div class="main-container">
		<!-- æ–°é—»æ ‡é¢˜ -->
		<div class="news-header">
			<!-- è¿”å›æŒ‰é’® -->
			<div class="back-button-container">
				<a href="../pages/news.html" class="back-button" title="Back to News">
					<svg viewBox="0 0 24 24" width="24" height="24" fill="currentColor">
						<path d="M20 11H7.83l5.59-5.59L12 4l-8 8 8 8 1.41-1.41L7.83 13H20v-2z"/>
					</svg>
					<span>Back to News</span>
				</a>
			</div>
			<h1 class="news-title">ğŸ‰ Multiple Papers of Our Team Have Been Accepted by CVPR 2022</h1>
			<div class="news-meta">
				<div class="news-date">
					ğŸ“… <span>March 2, 2022</span>
				</div>
				<div class="news-author">
					â±ï¸ <span>1 min read</span>
				</div>
				<span class="news-badge">CVPR 2022</span>
			</div>
		</div>
		
		<!-- æ–°é—»å›¾ç‰‡ -->
		<div class="news-image">
			<img src="../public/images/news/Multiple Papers of Our Team Have Been Accepted by CVPR 2022.webp" alt="CVPR 2022">
		</div>
		
		<!-- æ–°é—»å†…å®¹ -->
		<div class="news-content">
			<p>
				We are thrilled to announce that <strong>CVPR 2022</strong> has officially released the list of accepted papers, and <strong>multiple papers from our team are included</strong>! This is an outstanding achievement that showcases the exceptional research quality and innovation of our laboratory.
			</p>
			
			<h2>About CVPR 2022</h2>
			<p>
				The <strong>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</strong> is the premier annual computer vision event, bringing together researchers and practitioners from around the world. CVPR is widely recognized as one of the <strong>top-tier conferences</strong> in computer vision and artificial intelligence, with extremely competitive acceptance rates.
			</p>
			<p>
				Papers accepted by CVPR represent cutting-edge research that advances the state-of-the-art in computer vision, pattern recognition, and machine learning applications.
			</p>
			
			<h2>Paper 1: åŸºäºæ ·ä¾‹æŸ¥è¯¢æœºåˆ¶çš„åœ¨çº¿åŠ¨ä½œæ£€æµ‹</h2>
			<h2>Online Action Detection with Exemplar Query Mechanism</h2>
			
			<div class="paper-info">
				<h3>ğŸ“„ åŸºäºæ ·ä¾‹æŸ¥è¯¢æœºåˆ¶çš„åœ¨çº¿åŠ¨ä½œæ£€æµ‹ (Online Action Detection with Exemplar Query)</h3>
				<p><strong>Authors:</strong> Le Yang (æ¨ä¹), Junwei Han (éŸ©å†›ä¼Ÿ), Dingwen Zhang (å¼ é¼æ–‡)</p>
				<p><strong>Conference:</strong> IEEE/CVF Conference on Computer Vision and Pattern Recognition 2022</p>
			</div>
			
			<h3>Research Background</h3>
			<p>
				<strong>Temporal action localization</strong> aims to discover meaningful action segments from lengthy videos and annotate their start and end times along with action categories. This task condenses effective information from videos, serving video understanding tasks such as intelligent surveillance and content analysis.
			</p>
			<p>
				Considering practical applications where algorithms need to <strong>process video streams online</strong> and detect ongoing actions in videos <strong>timely and accurately</strong>, online action detection has emerged as a new research direction with significant real-world importance.
			</p>
			
			<h3>Key Innovation: Exemplar Query Mechanism</h3>
			<p>
				Existing methods only consider information from fixed historical segments and cannot model cross-video relationships. To address these limitations, this paper proposes an <strong>exemplar query mechanism</strong>:
			</p>
			<ul>
				<li><strong>Historical Segment Exemplars</strong>: Constructing representative exemplars from historical segments for online action detection</li>
				<li><strong>Action Category-Level Exemplars</strong>: Building exemplars at the action category level to capture cross-video patterns</li>
				<li><strong>Efficient Processing</strong>: The proposed method runs faster than existing approaches</li>
				<li><strong>Superior Accuracy</strong>: Achieves higher detection accuracy compared to state-of-the-art methods</li>
			</ul>
			
		<h3>Significance</h3>
		<p>
			This work provides a <strong>simple yet effective baseline model</strong> for future research in online action detection, offering both efficiency and accuracy improvements that are crucial for real-world video understanding applications.
		</p>
		
		<!-- è®ºæ–‡1é…å›¾ -->
		<div class="news-image">
			<img src="../public/images/news/Multiple Papers of Our Team Have Been Accepted by CVPR 2022_1.png" alt="Online Action Detection">
		</div>
		
		<h2>Paper 2: åŸºäºå¢é‡è·¨è§†å›¾äº’è’¸é¦å­¦ä¹ æœºåˆ¶çš„CTå½±åƒç”Ÿæˆ</h2>
			<h2>CT Image Synthesis via Incremental Cross-View Mutual Distillation</h2>
			
		<div class="paper-info">
			<h3>ğŸ“„ åŸºäºå¢é‡è·¨è§†å›¾äº’è’¸é¦å­¦ä¹ æœºåˆ¶çš„CTå½±åƒç”Ÿæˆ (Incremental Cross-View Mutual Distillation for CT Synthesis)</h3>
			<p><strong>Authors:</strong> Chaowei Fang (æ–¹è¶…ä¼Ÿ), Liang Wang (ç‹è‰¯), Jun Xu (å¾å›), Yixuan Yuan (è¢å¥•è±), Dingwen Zhang (å¼ é¼æ–‡), Junwei Han (éŸ©å†›ä¼Ÿ)</p>
			<p><strong>Conference:</strong> IEEE/CVF Conference on Computer Vision and Pattern Recognition 2022</p>
		</div>
		
		<!-- è®ºæ–‡2é…å›¾1 -->
		<div class="news-image">
			<img src="../public/images/news/Multiple Papers of Our Team Have Been Accepted by CVPR 2022_2.png" alt="CT Image Synthesis">
		</div>
		
		<h3>Research Background</h3>
			<p>
				<strong>High-resolution CT images</strong> can help doctors and medical AI systems perform accurate imaging analysis and disease diagnosis. However, due to the characteristics of human body structure, it is difficult to obtain sufficiently high <strong>inter-slice resolution</strong> for CT images in the axial view.
			</p>
			
			<h3>Key Innovation: Incremental Cross-View Mutual Distillation</h3>
			<p>
				This paper constructs a <strong>self-supervised axial view CT slice generation method</strong> and proposes an <strong>incremental cross-view mutual distillation learning mechanism</strong>:
			</p>
			<ul>
				<li><strong>High-Resolution Prior Exploitation</strong>: Utilizing high-resolution priors from sagittal and coronal view images</li>
				<li><strong>Consistency Constraints</strong>: Establishing consistency constraints between different views</li>
				<li><strong>Joint Iterative Interpolation</strong>: Jointly iterating the image interpolation process across different views</li>
				<li><strong>Incremental Resolution Enhancement</strong>: Achieving incremental improvement in axial view inter-slice resolution</li>
				<li><strong>Improved Robustness</strong>: Enhancing the model's robustness to handle CT images with different slice thicknesses</li>
			</ul>
			
		<h3>Clinical Significance</h3>
		<p>
			This research addresses a critical challenge in medical imaging by enabling:
		</p>
		<ul>
			<li>More accurate disease diagnosis through higher-resolution axial CT images</li>
			<li>Better handling of CT scans with varying slice thicknesses</li>
			<li>Improved medical image analysis without additional scanning time or radiation exposure</li>
			<li>Enhanced compatibility with existing CT imaging protocols</li>
		</ul>
		
		<!-- è®ºæ–‡2é…å›¾2 -->
		<div class="news-image">
			<img src="../public/images/news/Multiple Papers of Our Team Have Been Accepted by CVPR 2022_3.png" alt="CT Synthesis Results">
		</div>
		
		<h2>Paper 3: åŸºäºé²æ£’åŒºåŸŸç‰¹å¾ç”Ÿæˆçš„é›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹</h2>
			<h2>Zero-Shot Object Detection via Robust Region Feature Synthesis</h2>
			
			<div class="paper-info">
				<h3>ğŸ“„ åŸºäºé²æ£’åŒºåŸŸç‰¹å¾ç”Ÿæˆçš„é›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹ (Robust Region Feature Synthesizer for Zero-Shot Object Detection)</h3>
				<p><strong>Authors:</strong> Peiliang Huang (é»„åŸ¹äº®), Junwei Han (éŸ©å†›ä¼Ÿ), De Cheng (ç¨‹å¾·), Dingwen Zhang (å¼ é¼æ–‡)</p>
				<p><strong>Conference:</strong> IEEE/CVF Conference on Computer Vision and Pattern Recognition 2022</p>
			</div>
			
			<h3>Research Background</h3>
			<p>
				<strong>Zero-shot object detection</strong> aims to enhance a model's ability to detect object classes that are not visible during the training phase. This is a crucial capability for building generalizable AI systems that can recognize novel objects without requiring extensive labeled training data.
			</p>
			
			<h3>Key Challenges</h3>
			<p>
				Traditional zero-shot learning models face significant difficulties in the object detection task:
			</p>
			<ul>
				<li>Difficulty generating region features with sufficient <strong>intra-class diversity</strong> for unseen objects</li>
				<li>Potential sacrifice of <strong>discriminability</strong> between unseen objects and image backgrounds</li>
			</ul>
			
			<h3>Key Innovation: Robust Region Feature Synthesis</h3>
			<p>
				Fully considering the uniqueness of the object detection task, this research proposes to utilize the rich foreground and background region features contained in training images to <strong>simultaneously maintain intra-class diversity and inter-class discriminability</strong> of unseen object features:
			</p>
			<ul>
				<li><strong>Unified Detection Model</strong>: First to achieve a unified object detection model for both seen and unseen object classes simultaneously</li>
				<li><strong>Rich Feature Utilization</strong>: Leveraging abundant foreground-background region features from training images</li>
				<li><strong>Intra-class Diversity</strong>: Maintaining diversity within unseen object classes for robust detection</li>
				<li><strong>Inter-class Discriminability</strong>: Preserving discriminative power between different object classes and backgrounds</li>
				<li><strong>Novel Benchmark</strong>: Providing the first benchmark for zero-shot remote sensing object detection</li>
			</ul>
			
		<h3>Research Significance</h3>
		<p>
			This work makes several important contributions:
		</p>
		<ul>
			<li>ğŸ¯ <strong>Unified Framework</strong>: First unified detection model handling both seen and unseen classes</li>
			<li>ğŸ›°ï¸ <strong>Remote Sensing Benchmark</strong>: Pioneering work in zero-shot remote sensing object detection</li>
			<li>ğŸ”¬ <strong>Novel Approach</strong>: Innovative region feature synthesis that balances diversity and discriminability</li>
			<li>ğŸŒŸ <strong>Practical Impact</strong>: Enabling object detection systems to generalize to new object categories without retraining</li>
		</ul>
		
		<!-- è®ºæ–‡3é…å›¾ -->
		<div class="news-image">
			<img src="../public/images/news/Multiple Papers of Our Team Have Been Accepted by CVPR 2022_4.jpg" alt="Zero-Shot Object Detection">
		</div>
		
		<h2>Overall Impact</h2>
			<p>
				The acceptance of these three papers by <strong>CVPR 2022</strong> demonstrates our laboratory's research excellence across multiple important areas:
			</p>
			<ul>
				<li><strong>Video Understanding</strong>: Online action detection for real-time video analysis</li>
				<li><strong>Medical Imaging</strong>: CT image synthesis for improved clinical diagnosis</li>
				<li><strong>Zero-Shot Learning</strong>: Novel object detection without extensive training data</li>
			</ul>
			<p>
				These diverse research directions showcase our team's capability to address critical challenges in computer vision and its applications to real-world problems in surveillance, healthcare, and beyond.
			</p>
			
			<h2>Research Applications</h2>
			<p>
				The technologies developed in these papers have broad practical applications:
			</p>
			<ul>
				<li>ğŸ¥ <strong>Intelligent Surveillance</strong>: Real-time action detection for security and monitoring systems</li>
				<li>ğŸ¥ <strong>Medical Diagnosis</strong>: Enhanced CT imaging for more accurate disease detection</li>
				<li>ğŸ›°ï¸ <strong>Remote Sensing</strong>: Zero-shot object detection for satellite and aerial imagery analysis</li>
				<li>ğŸ“¹ <strong>Content Analysis</strong>: Automated video content understanding and indexing</li>
				<li>ğŸ¤– <strong>AI Generalization</strong>: Building more adaptable AI systems that can recognize novel objects</li>
			</ul>
			
			<h2>Conclusion</h2>
			<p>
				We are extremely proud of our team's achievement in having <strong>multiple papers accepted by CVPR 2022</strong>, one of the most prestigious and competitive conferences in computer vision. These acceptances reflect the high quality, innovation, and impact of our research.
			</p>
			<p>
				The three accepted papers span diverse and important research areas, from real-time video understanding to medical image synthesis and zero-shot object detection. This diversity demonstrates our laboratory's comprehensive research capabilities and our commitment to advancing computer vision technology for real-world applications.
			</p>
			<p>
				Congratulations to all team members involved in these outstanding research projects! ğŸŠ
			</p>
		</div>
	</div>
	
	<!-- é¡µè„š -->
	<footer class="footer">
		<div class="footer-container">
			<p>Copyright Â© 2025 <a href="../index.html">Brain Lab</a>. All rights reserved.</p>
			<p style="font-size: 0.9rem; margin-top: 0.75rem;">
				è„‘ä¸äººå·¥æ™ºèƒ½å®éªŒå®¤ | Vision Intelligence Research Group
			</p>
		</div>
	</footer>
</body>
</html>

