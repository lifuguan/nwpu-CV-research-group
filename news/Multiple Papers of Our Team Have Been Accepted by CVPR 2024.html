<!doctype html>
<html lang="zh-CN">
<head>
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Multiple Papers of Our Team Have Been Accepted by CVPR 2024 - Vision Intelligence Research Group</title>
	
	<meta name='robots' content='max-image-preview:large' />
	<meta property="og:type" content="website" />
	<meta property="og:title" content="Vision Intelligence Research Group" />
	
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=Playfair+Display:wght@600;700;800&display=swap" rel="stylesheet">
	
	<style>
		/* ==================== ÂÖ®Â±ÄÂèòÈáè ==================== */
		:root {
			--primary-color: #2563eb;
			--primary-dark: #1e40af;
			--accent-color: #10b981;
			
			--text-white: #ffffff;
			--text-dark: #1e293b;
			--text-gray: #64748b;
			--text-light: #94a3b8;
			
			--bg-white: #ffffff;
			--bg-light: #f8fafc;
			--bg-dark: #0f172a;
			
			--border-color: #e2e8f0;
			
			--shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
			--shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
			--shadow-lg: 0 20px 25px -5px rgba(0, 0, 0, 0.1);
			
			--transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
		}
		
		* {
			margin: 0;
			padding: 0;
			box-sizing: border-box;
		}
		
		body {
			font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
			color: var(--text-dark);
			line-height: 1.8;
			overflow-x: hidden;
			background: var(--bg-light);
		}
		
		/* ==================== ÂØºËà™Ê†è ==================== */
		.navbar {
			background: rgba(255, 255, 255, 0.98);
			backdrop-filter: blur(10px);
			box-shadow: var(--shadow-sm);
			position: sticky;
			top: 0;
			left: 0;
			right: 0;
			z-index: 1000;
			padding: 1rem 2rem;
			transition: var(--transition);
		}
		
		.nav-container {
			max-width: 1400px;
			margin: 0 auto;
			display: flex;
			justify-content: space-between;
			align-items: center;
		}
		
		.logo {
			font-family: 'Playfair Display', serif;
			font-size: 1.5rem;
			font-weight: 800;
			color: var(--text-dark);
			text-decoration: none;
			transition: var(--transition);
		}
		
		.nav-menu {
			display: flex;
			gap: 2.5rem;
			list-style: none;
			align-items: center;
		}
		
		.nav-menu a {
			color: var(--text-gray);
			text-decoration: none;
			font-weight: 500;
			font-size: 0.95rem;
			transition: var(--transition);
			position: relative;
		}
		
		.nav-menu a:hover {
			color: var(--primary-color);
		}
		
	/* ==================== ‰∏ªÂÆπÂô® ==================== */
	.main-container {
		max-width: 1200px;
		margin: 0 auto;
		padding: 3rem 2rem;
	}
		
	/* ==================== ËøîÂõûÊåâÈíÆ ==================== */
	.back-button-container {
		position: absolute;
		left: -80px;
		top: 3rem;
	}
	
	.back-button {
		display: inline-flex;
		align-items: center;
		justify-content: center;
		width: 60px;
		height: 60px;
		background: var(--bg-white);
		border: 2px solid var(--border-color);
		border-radius: 50%;
		color: var(--text-gray);
		text-decoration: none;
		transition: var(--transition);
		box-shadow: var(--shadow-md);
	}
	
	.back-button:hover {
		background: var(--primary-color);
		color: var(--text-white);
		border-color: var(--primary-color);
		transform: scale(1.1);
		box-shadow: var(--shadow-lg);
	}
	
	.back-button svg {
		transition: var(--transition);
	}
	
	.back-button:hover svg {
		transform: translateX(-3px);
	}
	
	.back-button span {
		display: none;
	}
		
	/* ==================== Êñ∞ÈóªÂ§¥ÈÉ® ==================== */
	.news-header {
		background: white;
		padding: 2.5rem;
		border-radius: 16px;
		box-shadow: var(--shadow-md);
		margin-bottom: 2rem;
		position: relative;
	}
		
		.news-title {
			font-family: 'Playfair Display', serif;
			font-size: 2.5rem;
			font-weight: 800;
			color: var(--text-dark);
			line-height: 1.3;
			margin-bottom: 1.5rem;
		}
		
		.news-meta {
			display: flex;
			flex-wrap: wrap;
			gap: 1.5rem;
			color: var(--text-gray);
			font-size: 0.95rem;
		}
		
		.news-date {
			display: flex;
			align-items: center;
			gap: 0.5rem;
		}
		
		.news-author {
			display: flex;
			align-items: center;
			gap: 0.5rem;
		}
		
		.news-badge {
			display: inline-block;
			padding: 0.5rem 1.25rem;
			background: linear-gradient(135deg, #f59e0b, #d97706);
			color: white;
			border-radius: 25px;
			font-weight: 700;
			font-size: 0.85rem;
			letter-spacing: 0.5px;
		}
		
		/* ==================== Êñ∞ÈóªÂõæÁâá ==================== */
		.news-image {
			width: 100%;
			margin-bottom: 2rem;
			border-radius: 16px;
			overflow: hidden;
			box-shadow: var(--shadow-md);
		}
		
		.news-image img {
			width: 100%;
			height: auto;
			display: block;
		}
		
		/* ==================== Êñ∞ÈóªÂÜÖÂÆπ ==================== */
		.news-content {
			background: white;
			padding: 2.5rem;
			border-radius: 16px;
			box-shadow: var(--shadow-md);
		}
		
		.news-content h2 {
			font-family: 'Playfair Display', serif;
			font-size: 1.75rem;
			font-weight: 700;
			color: var(--text-dark);
			margin: 2rem 0 1rem 0;
			padding-bottom: 0.5rem;
			border-bottom: 3px solid var(--primary-color);
		}
		
		.news-content h2:first-child {
			margin-top: 0;
		}
		
		.news-content h3 {
			font-size: 1.4rem;
			font-weight: 600;
			color: var(--text-dark);
			margin: 1.5rem 0 1rem 0;
		}
		
		.news-content p {
			margin-bottom: 1.25rem;
			color: var(--text-gray);
			font-size: 1.05rem;
		}
		
		.news-content strong {
			color: var(--text-dark);
			font-weight: 600;
		}
		
		.news-content a {
			color: var(--primary-color);
			text-decoration: none;
			transition: var(--transition);
		}
		
		.news-content a:hover {
			color: var(--primary-dark);
			text-decoration: underline;
		}
		
		.paper-info {
			background: linear-gradient(135deg, #eff6ff, #dbeafe);
			padding: 1.5rem;
			border-radius: 12px;
			border-left: 4px solid var(--primary-color);
			margin: 1.5rem 0;
		}
		
		.paper-info h3 {
			color: var(--primary-dark);
			margin-top: 0;
			font-size: 1.3rem;
		}
		
		.paper-info p {
			margin-bottom: 0.5rem;
		}
		
		/* ==================== È°µËÑö ==================== */
		.footer {
			background: var(--bg-dark);
			color: var(--text-white);
			padding: 3rem 2rem;
			margin-top: 4rem;
			text-align: center;
		}
		
		.footer-container {
			max-width: 1400px;
			margin: 0 auto;
		}
		
		.footer-container p {
			color: var(--text-light);
			margin: 0.5rem 0;
		}
		
		.footer-container a {
			color: var(--accent-color);
			text-decoration: none;
		}
		
		/* ==================== ÂìçÂ∫îÂºèËÆæËÆ° ==================== */
		@media (max-width: 768px) {
			.news-title {
				font-size: 2rem;
			}
			
			.news-meta {
				flex-direction: column;
				gap: 0.75rem;
			}
			
		.main-container {
			padding: 2rem 1rem;
		}
		
		.back-button-container {
			position: static;
			transform: none;
			margin-bottom: 1rem;
		}
		
		.back-button {
			width: 50px;
			height: 50px;
		}
			
			.news-header,
			.news-content {
				padding: 1.5rem;
			}
		}
	</style>
</head>
<body>
	<!-- ÂØºËà™Ê†è -->
	<nav class="navbar" id="navbar">
		<div class="nav-container">
			<a href="../index.html" class="logo">Vision Intelligence Group</a>
						<ul class="nav-menu">
				<li><a href="../index.html">Home</a></li>
				<li><a href="../pages/people.html">People</a></li>
				<li><a href="../pages/publications.html">Publications</a></li>
				<li><a href="../pages/demo.html">Projects</a></li>
				<li><a href="../pages/news.html">News</a></li>
				<li><a href="../pages/event.html">Events</a></li>
			</ul>
		</div>
	</nav>
	
<div class="main-container">
	<!-- Êñ∞ÈóªÊ†áÈ¢ò -->
	<div class="news-header">
		<!-- ËøîÂõûÊåâÈíÆ -->
		<div class="back-button-container">
			<a href="../pages/news.html" class="back-button" title="Back to News">
				<svg viewBox="0 0 24 24" width="24" height="24" fill="currentColor">
					<path d="M20 11H7.83l5.59-5.59L12 4l-8 8 8 8 1.41-1.41L7.83 13H20v-2z"/>
				</svg>
				<span>Back to News</span>
			</a>
		</div>
		<h1 class="news-title">üéâ Multiple Papers of Our Team Have Been Accepted by CVPR 2024</h1>
		<div class="news-meta">
			<div class="news-date">
				üìÖ <span>February 27, 2024</span>
			</div>
			<div class="news-author">
				‚è±Ô∏è <span>2 min read</span>
			</div>
			<span class="news-badge">CVPR 2024</span>
		</div>
	</div>
		
		<!-- Êñ∞ÈóªÂõæÁâá -->
		<div class="news-image">
			<img src="../public/images/news/Multiple Papers of Our Team Have Been Accepted by CVPR 2024.webp" alt="CVPR 2024">
		</div>
		
		<!-- Êñ∞ÈóªÂÜÖÂÆπ -->
		<div class="news-content">
			<p>
				<strong>CVPR 2024</strong> officially released the list of accepted papers. We are thrilled to announce that multiple papers from our team have been included!
			</p>
			
			<h2>Paper 1: GP-NeRF</h2>
			
			<div class="paper-info">
				<h3>üìÑ GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding</h3>
				<p><strong>Authors:</strong> Hao Li, Dingwen Zhang, Yalun Dai, et al.</p>
				<p><strong>Conference:</strong> CVPR 2024</p>
			</div>
			
			<h3>Research Background</h3>
			<p>
				Applying <strong>NeRF to downstream perception tasks</strong> for scene understanding and representation is becoming increasingly popular. Most existing methods treat semantic prediction as an additional rendering task, i.e., the "label rendering" task, to build semantic NeRFs.
			</p>
			<p>
				However, by rendering semantic/instance labels per pixel without considering the contextual information of the rendered image, these methods usually suffer from <strong>unclear boundary segmentation</strong> and <strong>abnormal segmentation of pixels</strong> within an object.
			</p>
			
			<h3>Key Contributions</h3>
			<p>
				To solve this problem, we propose <strong>Generalized Perception NeRF (GP-NeRF)</strong>, a novel pipeline that makes the widely used segmentation model and NeRF work compatibly under a unified framework, for facilitating context-aware 3D scene perception.
			</p>
			<p>
				<strong>Main innovations include:</strong>
			</p>
			<ul style="margin-left: 2rem; margin-bottom: 1.25rem;">
				<li style="margin-bottom: 0.5rem;">We introduce <strong>transformers</strong> to aggregate radiance as well as semantic embedding fields jointly for novel views and facilitate the joint volumetric rendering of both fields.</li>
				<li style="margin-bottom: 0.5rem;">We propose two self-distillation mechanisms: the <strong>Semantic Distill Loss</strong> and the <strong>Depth-Guided Semantic Distill Loss</strong>, to enhance the discrimination and quality of the semantic field and the maintenance of geometric consistency.</li>
			</ul>
			
			<h3>Experimental Results</h3>
			<p>
				We conduct experimental comparisons under two perception tasks (semantic and instance segmentation) using both synthetic and real-world datasets. Notably, our method <strong>outperforms SOTA approaches</strong> by:
			</p>
			<ul style="margin-left: 2rem; margin-bottom: 1.25rem;">
				<li><strong>6.94%</strong> on generalized semantic segmentation</li>
				<li><strong>11.76%</strong> on finetuning semantic segmentation</li>
				<li><strong>8.47%</strong> on instance segmentation</li>
			</ul>
			
			<h2>Paper 2: LTGC</h2>
			
			<div class="paper-info">
				<h3>üìÑ LTGC: Long-Tail Recognition via Leveraging Generated Content</h3>
				<p><strong>Authors:</strong> Qihao Zhao, Yalun Dai, Hao Li, Wei Hu, Fan Zhang, Jun Liu</p>
				<p><strong>Conference:</strong> CVPR 2024</p>
			</div>
			
			<h3>Research Background</h3>
			<p>
				<strong>Long-tail recognition</strong> is challenging because it requires the model to learn good representations from tail categories and address imbalances across all categories.
			</p>
			
			<h3>Key Contributions</h3>
			<p>
				In this paper, we propose a novel <strong>generative and fine-tuning framework, LTGC</strong>, to handle long-tail recognition via leveraging generated content.
			</p>
			<p>
				<strong>Main innovations include:</strong>
			</p>
			<ul style="margin-left: 2rem; margin-bottom: 1.25rem;">
				<li style="margin-bottom: 0.5rem;">Inspired by the rich implicit knowledge in <strong>large-scale models</strong> (e.g., large language models, LLMs), LTGC leverages the power of these models to parse and reason over the original tail data to produce diverse tail-class content.</li>
				<li style="margin-bottom: 0.5rem;">We propose several novel designs for LTGC to ensure the <strong>quality of the generated data</strong> and to efficiently fine-tune the model using both the generated and original data.</li>
			</ul>
			
			<h3>Experimental Results</h3>
			<p>
				The visualization demonstrates the effectiveness of the generation module in LTGC, which produces <strong>accurate and diverse tail data</strong>. Additionally, the experimental results demonstrate that our LTGC <strong>outperforms existing state-of-the-art methods</strong> on popular long-tailed benchmarks.
			</p>
			
			<h2>Conclusion</h2>
			<p>
				These acceptances at CVPR 2024 represent significant milestones for our research team. Both papers tackle important challenges in computer vision - one focusing on 3D scene understanding with NeRF, and the other addressing the long-tail recognition problem with generative models.
			</p>
			<p>
				Congratulations to Hao Li, Qihao Zhao, and all co-authors for these outstanding achievements! üéä
			</p>
		</div>
	</div>
	
	<!-- È°µËÑö -->
	<footer class="footer">
		<div class="footer-container">
			<p>Copyright ¬© 2025 <a href="../index.html">Brain Lab</a>. All rights reserved.</p>
			<p style="font-size: 0.9rem; margin-top: 0.75rem;">
				ËÑë‰∏é‰∫∫Â∑•Êô∫ËÉΩÂÆûÈ™åÂÆ§ | Vision Intelligence Research Group
			</p>
		</div>
	</footer>
</body>
</html>


